{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests                   \n",
    "from bs4 import BeautifulSoup     \n",
    "import pandas as pd               \n",
    "import time                       \n",
    "import numpy as np                \n",
    "import re         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2000-2020 - e.g 2000 nfl draft is based on 1999 college season\n",
    "years_nfl = ['2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015','2016','2017','2018','2019', '2020']\n",
    "\n",
    "# 1999-2019 - e.g 1999 season ends in 2000 and players get drafted in 2000\n",
    "years_ncaa = ['1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015','2016','2017','2018','2019']\n",
    "\n",
    "# list of tables for player stats\n",
    "player_tables = ['passing','rushing','receiving','kicking','punting']\n",
    "\n",
    "# list of delays to put between get requests\n",
    "delays = [5, 6, 7, 8, 9, 10, 14, 17, 19, 23, 27, 29, 31, 33, 35, 42, 45]\n",
    "\n",
    "# set request headers based on website\n",
    "headers = {\n",
    "    'accept': '*/*',\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "    'referer': 'http://www.google.com/',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get combine data\n",
    "def pullCombineData(years):\n",
    "    # a list to store dataframes -- 1 df per year\n",
    "    dfs = []\n",
    "    # a list to store any errors that may come up while scraping\n",
    "    error_list = []\n",
    "    \n",
    "    # iterate over years\n",
    "    for year in years:\n",
    "        # use try/except block to catch and inspect any urls that cause an error\n",
    "        try:\n",
    "            # set url\n",
    "            url = 'https://www.pro-football-reference.com/draft/' + year + '-combine.htm'\n",
    "            \n",
    "            # put random delays between get requests\n",
    "            delay = np.random.choice(delays)\n",
    "            time.sleep(delay)\n",
    "            # make get request with headers\n",
    "            html = requests.get(url, headers=headers)\n",
    "\n",
    "            # create the BeautifulSoup object\n",
    "            soup = BeautifulSoup(html.content, \"lxml\")\n",
    "            # get combine table\n",
    "            table = soup.find('table', {'id': 'combine'})\n",
    "            # get column headers\n",
    "            column_headers = [th.getText() for th in soup.findAll('tr', limit=2)[0].findAll('th')]\n",
    "            \n",
    "            # read table as dataframe\n",
    "            df = pd.read_html(str(table))[0]\n",
    "            # remove duplicate header rows\n",
    "            df.drop_duplicates(keep=False, inplace=True) \n",
    "\n",
    "            # get player ids and ncaa links, if not exist put N/A\n",
    "            player_ids = []\n",
    "            ncaa_links = []\n",
    "            for tbody in table.find_all('tbody'):\n",
    "                for tr in tbody.find_all('tr'):\n",
    "                    for th in tr.find_all('th'):\n",
    "                        if th.text not in column_headers:\n",
    "                            try:\n",
    "                                player_ids.append(th['data-append-csv'])\n",
    "                            except KeyError:\n",
    "                                player_ids.append('N/A')\n",
    "                    for td in tr.find_all('td'):\n",
    "                            if td['data-stat'] == \"college\":\n",
    "                                if td.find_all('a'):\n",
    "                                    for a in td.find_all('a'):\n",
    "                                        ncaa_links.append(a['href'])\n",
    "                                else:\n",
    "                                    ncaa_links.append('N/A')\n",
    "            \n",
    "            # insert year, player id, ncaa link to table\n",
    "            df.insert(0, \"NCAA_Link\", ncaa_links)\n",
    "            df.insert(0, \"Player_ID\", player_ids)\n",
    "            df.insert(0, \"Year\", year)\n",
    "            \n",
    "            # append df to dfs list\n",
    "            dfs.append(df)\n",
    "\n",
    "        except Exception as e:\n",
    "            # store the url and the error it causes in a list\n",
    "            error = [url, e] \n",
    "            # append it to the list of errors\n",
    "            error_list.append(error)\n",
    "    \n",
    "    # print errors\n",
    "    print(error_list)\n",
    "    # merge into one DataFrame\n",
    "    combines_df = pd.concat(dfs, ignore_index=True)\n",
    "    combines_df.to_csv('nfl_combine_2000_2020.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "pullCombineData(years_nfl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get draft data\n",
    "def pullDraftData(years):\n",
    "    # a list to store dataframes -- 1 df per year\n",
    "    dfs = []\n",
    "    # a list to store any errors that may come up while scraping\n",
    "    error_list = []\n",
    "    \n",
    "    # iterate over years\n",
    "    for year in years:\n",
    "        # use try/except block to catch and inspect any urls that cause an error\n",
    "        try:\n",
    "            # set url\n",
    "            url = 'https://www.pro-football-reference.com/years/' + year + '/draft.htm'\n",
    "            \n",
    "            # put random delays between get requests\n",
    "            delay = np.random.choice(delays)\n",
    "            time.sleep(delay)\n",
    "            # make get request with header\n",
    "            html = requests.get(url, headers=headers)\n",
    "\n",
    "            # create the BeautifulSoup object\n",
    "            soup = BeautifulSoup(html.content, \"lxml\")\n",
    "            # get draft table\n",
    "            table = soup.find('table', {'id': 'drafts'})\n",
    "            \n",
    "            # read table as dataframe\n",
    "            df = pd.read_html(str(table))[0]\n",
    "            # remove duplicate header rows\n",
    "            df.drop_duplicates(keep=False, inplace=True)\n",
    "            # remove multiheaders (first header row)\n",
    "            df.columns = df.columns.droplevel(0)\n",
    "\n",
    "            player_ids = []\n",
    "            ncaa_links = []\n",
    "            for tbody in table.find_all('tbody'):\n",
    "                for tr in tbody.find_all('tr'):\n",
    "                    for td in tr.find_all('td'):\n",
    "                        if td['data-stat'] == \"college_link\":\n",
    "                            if td.find_all('a'):\n",
    "                                for a in td.find_all('a'):\n",
    "                                    ncaa_links.append(a['href'])\n",
    "                            else:\n",
    "                                ncaa_links.append('N/A')\n",
    "                        elif td['data-stat'] == \"player\":\n",
    "                            try:\n",
    "                                player_ids.append(td['data-append-csv'])\n",
    "                            except KeyError:\n",
    "                                player_ids.append('N/A')\n",
    "                            \n",
    "            # insert year, player id, ncaa link to table\n",
    "            df.insert(0, \"NCAA_Link\", ncaa_links)\n",
    "            df.insert(0, \"Player_ID\", player_ids)\n",
    "            df.insert(0, \"Year\", year)\n",
    "            \n",
    "            # append df to dfs list\n",
    "            dfs.append(df)\n",
    "\n",
    "        except Exception as e:\n",
    "            # store the url and the error it causes in a list\n",
    "            error = [url, e] \n",
    "            # append it to the list of errors\n",
    "            error_list.append(error)\n",
    "    \n",
    "    # print errors\n",
    "    print(error_list)\n",
    "    # concatenate dfs into a single df\n",
    "    draft_df = pd.concat(dfs, ignore_index=True)\n",
    "    draft_df.to_csv('nfl_draft_2000_2020.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "pullDraftData(years_nfl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ncaa team offense data\n",
    "def pullTeamOffData(years):    \n",
    "    # a list to store dataframes -- 1 df per year\n",
    "    dfs = []\n",
    "    # a list to store any errors that may come up while scraping\n",
    "    error_list = []\n",
    "    \n",
    "    # iterate over years\n",
    "    for year in years:\n",
    "        # use try/except block to catch and inspect any urls that cause an error\n",
    "        try:\n",
    "            # set url\n",
    "            url = 'https://www.sports-reference.com/cfb/years/' + year + '-team-offense.html'\n",
    "            \n",
    "            # put random delays between get requests\n",
    "            delay = np.random.choice(delays)\n",
    "            time.sleep(delay)\n",
    "            # make get request with header\n",
    "            html = requests.get(url, headers=headers)\n",
    "\n",
    "            # create the BeautifulSoup object\n",
    "            soup = BeautifulSoup(html.content, \"lxml\")\n",
    "            \n",
    "            # get team offense table\n",
    "            table = soup.find('table', {'id': 'offense'})\n",
    "            \n",
    "            # read table as dataframe\n",
    "            df = pd.read_html(str(table))[0]\n",
    "            # remove duplicate header rows\n",
    "            df.drop_duplicates(keep=False, inplace=True)\n",
    "            # remove multilevel headers (first header row)\n",
    "            df.columns = df.columns.droplevel(0)\n",
    "            \n",
    "            # add year\n",
    "            df.insert(0, \"Year\", year)\n",
    "            \n",
    "            # append df to dfs list\n",
    "            dfs.append(df)\n",
    "        \n",
    "        except Exception as e:\n",
    "            # store the url and the error it causes in a list\n",
    "            error = [url, e] \n",
    "            # append it to the list of errors\n",
    "            error_list.append(error)\n",
    "    \n",
    "    # print errors\n",
    "    print(error_list)\n",
    "    # concatenate dataframes into a single dataframe\n",
    "    team_offense_df = pd.concat(dfs, ignore_index=True)\n",
    "    # save dataframe to csv\n",
    "    team_offense_df.to_csv('ncaaf_team_offense_1999_2019.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['https://www.sports-reference.com/cfb/years/1999-team-offense.html', ValueError('No tables found')]]\n"
     ]
    }
   ],
   "source": [
    "pullTeamOffData(years_ncaa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ncaa team defense data\n",
    "def pullTeamDefData(years):    \n",
    "    # a list to store dataframes -- 1 df per year\n",
    "    dfs = []\n",
    "    # a list to store any errors that may come up while scraping\n",
    "    error_list = []\n",
    "    \n",
    "    # iterate over years\n",
    "    for year in years:\n",
    "        # use try/except block to catch and inspect any urls that cause an error\n",
    "        try:\n",
    "            # set url\n",
    "            url = 'https://www.sports-reference.com/cfb/years/' + year + '-team-defense.html'\n",
    "            # put random delays between get requests\n",
    "            delay = np.random.choice(delays)\n",
    "            time.sleep(delay)\n",
    "            # get html\n",
    "            html = requests.get(url, headers=headers)\n",
    "\n",
    "            # create the BeautifulSoup object\n",
    "            soup = BeautifulSoup(html.content, \"lxml\")\n",
    "            \n",
    "            # get team defense table\n",
    "            table = soup.find('table', {'id': 'defense'})\n",
    "            df = pd.read_html(str(table))[0]\n",
    "            # remove duplicate header rows\n",
    "            df.drop_duplicates(keep=False, inplace=True)\n",
    "            # remove multilevel headers\n",
    "            df.columns = df.columns.droplevel(0)\n",
    "            \n",
    "            # add year\n",
    "            df.insert(0, \"Year\", year)\n",
    "            \n",
    "            # append dataframe to dataframes list\n",
    "            dfs.append(df)\n",
    "        \n",
    "        except Exception as e:\n",
    "            # store the url and the error it causes in a list\n",
    "            error = [url, e] \n",
    "            # append it to the list of errors\n",
    "            error_list.append(error)\n",
    "    \n",
    "    # print errors\n",
    "    print(error_list)\n",
    "    # concatenate dataframes into a single dataframe\n",
    "    team_defense_df = pd.concat(dfs, ignore_index=True)\n",
    "    # save dataframe to csv\n",
    "    team_defense_df.to_csv('ncaaf_team_defense_1999_2019.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['https://www.sports-reference.com/cfb/years/1999-team-defense.html', ValueError('No tables found')]]\n"
     ]
    }
   ],
   "source": [
    "pullTeamDefData(years_ncaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get NCAAF ratings\n",
    "def pullTeamRatings(years):    \n",
    "    # a list to store dataframes -- 1 df per year\n",
    "    dfs = []\n",
    "    # a list to store any errors that may come up while scraping\n",
    "    error_list = []\n",
    "    \n",
    "    # iterate over years\n",
    "    for year in years:\n",
    "        # use try/except block to catch and inspect any urls that cause an error\n",
    "        try:\n",
    "            # set url\n",
    "            url = 'https://www.sports-reference.com/cfb/years/' + year + '-ratings.html'\n",
    "            \n",
    "            # put random delays between get requests\n",
    "            delay = np.random.choice(delays)\n",
    "            time.sleep(delay)\n",
    "            # make get request with header\n",
    "            html = requests.get(url, headers=headers)\n",
    "\n",
    "            # create the BeautifulSoup object\n",
    "            soup = BeautifulSoup(html.content, \"lxml\")\n",
    "            \n",
    "            # get ratings table\n",
    "            table = soup.find('table', {'id': 'ratings'})\n",
    "            \n",
    "            # read table as dataframe\n",
    "            df = pd.read_html(str(table))[0]\n",
    "            # remove duplicate header rows\n",
    "            df.drop_duplicates(keep=False, inplace=True)\n",
    "            # remove multilevel headers (first header row)\n",
    "            df.columns = df.columns.droplevel(0)\n",
    "            \n",
    "            # add year\n",
    "            df.insert(0, \"Year\", year)\n",
    "            \n",
    "            # append df to dfs list\n",
    "            dfs.append(df)\n",
    "        \n",
    "        except Exception as e:\n",
    "            # store the url and the error it causes in a list\n",
    "            error = [url, e] \n",
    "            # append it to the list of errors\n",
    "            error_list.append(error)\n",
    "    \n",
    "    # print errors\n",
    "    print(error_list)\n",
    "    # concatenate dataframes into a single dataframe\n",
    "    team_ratings_df = pd.concat(dfs, ignore_index=True)\n",
    "    # save dataframe to csv\n",
    "    team_ratings_df.to_csv('ncaaf_team_ratings_1999_2019.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "pullTeamRatings(years_ncaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get nfl team stats\n",
    "def pullNFLTeamStats(years):    \n",
    "    # a list to store dataframes -- 1 df per year\n",
    "    dfs = []\n",
    "    # a list to store any errors that may come up while scraping\n",
    "    error_list = []\n",
    "    \n",
    "    # iterate over years\n",
    "    for year in years:\n",
    "        # use try/except block to catch and inspect any urls that cause an error\n",
    "        try:\n",
    "            # set url\n",
    "            url = 'https://www.pro-football-reference.com/years/' + year + '/'\n",
    "            # get html\n",
    "            delay = np.random.choice(delays)\n",
    "            time.sleep(delay)\n",
    "            html = requests.get(url, headers=headers)\n",
    "\n",
    "            # create the BeautifulSoup object\n",
    "            soup = BeautifulSoup(html.content, \"lxml\")\n",
    "            \n",
    "            # get NFL team stats\n",
    "            table = soup.find_all('table', {'id': ['AFC', 'NFC']})\n",
    "            # read table as dataframe\n",
    "            tables = pd.read_html(str(table))\n",
    "            \n",
    "            # concatenate tables into single dataframe\n",
    "            df = pd.concat(tables, ignore_index=True)\n",
    "            \n",
    "            # remove division rows\n",
    "            df = df[~df['Tm'].isin(divisions)]\n",
    "            \n",
    "            # add year\n",
    "            df.insert(0, \"Year\", year)\n",
    "            \n",
    "            # append dataframe to dataframes list\n",
    "            dfs.append(df)\n",
    "        \n",
    "        except Exception as e:\n",
    "            # store the url and the error it causes in a list\n",
    "            error = [url, e] \n",
    "            # append it to the list of errors\n",
    "            error_list.append(error)\n",
    "    \n",
    "    # print errors\n",
    "    print(error_list)\n",
    "    # concatenate dataframes into a single dataframe\n",
    "    nfl_team_stats_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # save dataframe to csv\n",
    "    nfl_team_stats_df.to_csv('nfl_team_stats_1999_2019.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['https://www.pro-football-reference.com/years/1999/', NameError(\"name 'divisions' is not defined\")], ['https://www.pro-football-reference.com/years/2000/', NameError(\"name 'divisions' is not defined\")], ['https://www.pro-football-reference.com/years/2001/', NameError(\"name 'divisions' is not defined\")], ['https://www.pro-football-reference.com/years/2002/', NameError(\"name 'divisions' is not defined\")], ['https://www.pro-football-reference.com/years/2003/', NameError(\"name 'divisions' is not defined\")], ['https://www.pro-football-reference.com/years/2004/', NameError(\"name 'divisions' is not defined\")], ['https://www.pro-football-reference.com/years/2005/', NameError(\"name 'divisions' is not defined\")], ['https://www.pro-football-reference.com/years/2006/', NameError(\"name 'divisions' is not defined\")], ['https://www.pro-football-reference.com/years/2007/', NameError(\"name 'divisions' is not defined\")], ['https://www.pro-football-reference.com/years/2008/', NameError(\"name 'divisions' is not defined\")], ['https://www.pro-football-reference.com/years/2009/', NameError(\"name 'divisions' is not defined\")], ['https://www.pro-football-reference.com/years/2010/', NameError(\"name 'divisions' is not defined\")], ['https://www.pro-football-reference.com/years/2011/', NameError(\"name 'divisions' is not defined\")], ['https://www.pro-football-reference.com/years/2012/', NameError(\"name 'divisions' is not defined\")], ['https://www.pro-football-reference.com/years/2013/', NameError(\"name 'divisions' is not defined\")], ['https://www.pro-football-reference.com/years/2014/', NameError(\"name 'divisions' is not defined\")], ['https://www.pro-football-reference.com/years/2015/', NameError(\"name 'divisions' is not defined\")], ['https://www.pro-football-reference.com/years/2016/', NameError(\"name 'divisions' is not defined\")], ['https://www.pro-football-reference.com/years/2017/', NameError(\"name 'divisions' is not defined\")], ['https://www.pro-football-reference.com/years/2018/', NameError(\"name 'divisions' is not defined\")], ['https://www.pro-football-reference.com/years/2019/', NameError(\"name 'divisions' is not defined\")]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-66f7fc68304c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpullNFLTeamStats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myears_ncaa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-838ae195d81e>\u001b[0m in \u001b[0;36mpullNFLTeamStats\u001b[0;34m(years)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# concatenate dataframes into a single dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mnfl_team_stats_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# save dataframe to csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIndexes\u001b[0m \u001b[0mhave\u001b[0m \u001b[0moverlapping\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \"\"\"\n\u001b[0;32m--> 274\u001b[0;31m     op = _Concatenator(\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0mobjs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No objects to concatenate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "pullNFLTeamStats(years_ncaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ncaa player data\n",
    "def pullPlayerData(years, tableID):\n",
    "    # a list to store dataframes -- 1 df per year\n",
    "    dfs = []\n",
    "    # a list to store any errors that may come up while scraping\n",
    "    error_list = []\n",
    "    # list of tables with multilevel headers\n",
    "    multi_list = ['passing','rushing','receiving']\n",
    "    \n",
    "    # iterate over years\n",
    "    for year in years:\n",
    "        # use try/except block to catch and inspect any urls that cause an error\n",
    "        try:\n",
    "            # set url\n",
    "            url = 'https://www.sports-reference.com/cfb/years/' + year + '-' + tableID + '.html'\n",
    "            \n",
    "            # put random delays between get requests\n",
    "            delay = np.random.choice(delays)\n",
    "            time.sleep(delay)\n",
    "            # make get request with header\n",
    "            html = requests.get(url, headers=headers)\n",
    "\n",
    "            # create the BeautifulSoup object\n",
    "            soup = BeautifulSoup(html.content, \"lxml\")\n",
    "            # get table\n",
    "            table = soup.find('table', {'id': tableID})\n",
    "            \n",
    "            # read table as dataframe\n",
    "            df = pd.read_html(str(table))[0]\n",
    "            # remove duplicate header rows\n",
    "            df.drop_duplicates(keep=False, inplace=True)\n",
    "            # remove multiheaders (first header row)\n",
    "            if tableID in multi_list:\n",
    "                df.columns = df.columns.droplevel(0)\n",
    "\n",
    "            player_ncaa_ids = []\n",
    "            ncaa_links = []\n",
    "            for tbody in table.find_all('tbody'):\n",
    "                for tr in tbody.find_all('tr'):\n",
    "                    for td in tr.find_all('td'):\n",
    "                        if td['data-stat'] == \"player\":\n",
    "                            try:\n",
    "                                player_ncaa_ids.append(td['data-append-csv'])\n",
    "                            except KeyError:\n",
    "                                player_ncaa_ids.append('N/A')\n",
    "                            \n",
    "                            if td.find_all('a'):\n",
    "                                for a in td.find_all('a'):\n",
    "                                    ncaa_links.append(a['href'])\n",
    "                            else:\n",
    "                                ncaa_links.append('N/A')\n",
    "                            \n",
    "            # insert year, player id, ncaa link to table\n",
    "            df.insert(0, \"NCAA_Link\", ncaa_links)\n",
    "            df.insert(0, \"Player_NCAA_ID\", player_ncaa_ids)\n",
    "            df.insert(0, \"Year\", year)\n",
    "            \n",
    "            # append df to dfs list\n",
    "            dfs.append(df)\n",
    "\n",
    "        except Exception as e:\n",
    "            # store the url and the error it causes in a list\n",
    "            error = [url, e] \n",
    "            # append it to the list of errors\n",
    "            error_list.append(error)\n",
    "    \n",
    "    # print errors\n",
    "    print(error_list)\n",
    "    # concatenate dataframes into a single dataframe\n",
    "    out_df = pd.concat(dfs, ignore_index=True)\n",
    "    out_df.to_csv('ncaa_player_' + tableID + '_stats_1999_2019.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in player_tables:\n",
    "    pullPlayerData(years, table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull All-Americans\n",
    "def pullAllAmericans(years):\n",
    "\n",
    "    base_url = 'https://www.sports-reference.com'\n",
    "    lst = []\n",
    "    \n",
    "    # a list to store any errors that may come up while scraping\n",
    "    error_list = []\n",
    "    \n",
    "    # iterate over years\n",
    "    for year in years:\n",
    "        # use try/except block to catch and inspect any urls that cause an error\n",
    "        try:\n",
    "            # set url\n",
    "            url = 'https://www.sports-reference.com/cfb/years/' + year + '.html'\n",
    "            \n",
    "            # put random delays between get requests \n",
    "            delay = np.random.choice(delays)\n",
    "            time.sleep(delay)\n",
    "            \n",
    "            # get html\n",
    "            res = requests.get(url, headers = headers)\n",
    "            \n",
    "            # Work around comments\n",
    "            comm = re.compile(\"<!--|-->\")\n",
    "            soup = BeautifulSoup(comm.sub(\"\", res.text), 'lxml')\n",
    "            for row in soup.find_all('div', id = 'div_all_americans'):\n",
    "                for p in row.find_all('p'):\n",
    "                    for a in p.find_all('a',limit=1):\n",
    "                        line = [year, p.text.split(',')[0].replace('*',''), p.text.split(',')[1].replace(' ',''), p.text.split(',')[2].lstrip(), base_url + a['href']]\n",
    "                    \n",
    "                    lst.append(line)\n",
    "        \n",
    "        except Exception as e:\n",
    "            # store the url and the error it causes in a list\n",
    "            error = [url, e] \n",
    "            # append it to the list of errors\n",
    "            error_list.append(error)\n",
    "    \n",
    "    # print errors\n",
    "    print(error_list)\n",
    "    # add list to dataframe\n",
    "    df=pd.DataFrame(lst,columns=['Year','Player','Pos','School','NCAA_Link'])\n",
    "    # save as csv\n",
    "    df.to_csv('ncaa_all_americans__1999_2019.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pullAllAmericans(years_ncaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PullNFLCombineResults(years):\n",
    "    # a list to store dataframes -- 1 df per year\n",
    "    dfs = []\n",
    "    # a list to store any errors that may come up while scraping\n",
    "    error_list = []\n",
    "    \n",
    "    # iterate over years\n",
    "    for year in years:\n",
    "        # use try/except block to catch and inspect any urls that cause an error\n",
    "        try:\n",
    "            # set url\n",
    "            url = 'https://nflcombineresults.com/nflcombinedata_expanded.php?year=' + year + ' &pos=&college='\n",
    "                        \n",
    "            # put random delays between get requests\n",
    "            delay = np.random.choice(delays)\n",
    "            time.sleep(delay)\n",
    "            # make get request with header\n",
    "            html = requests.get(url, headers=headers)\n",
    "\n",
    "            # create the BeautifulSoup object\n",
    "            soup = BeautifulSoup(html.content, \"lxml\")\n",
    "            \n",
    "            # get ratings table\n",
    "            table = soup.find('table', {'class': 'sortable'})\n",
    "            \n",
    "            # read table as dataframe\n",
    "            df = pd.read_html(str(table))[0]\n",
    "            \n",
    "            # append df to dfs list\n",
    "            dfs.append(df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # store the url and the error it causes in a list\n",
    "            error = [url, e] \n",
    "            # append it to the list of errors\n",
    "            error_list.append(error)\n",
    "    \n",
    "    # print errors\n",
    "    print(error_list)\n",
    "    # concatenate dataframes into a single dataframe\n",
    "    combineresults_df = pd.concat(dfs, ignore_index=True)\n",
    "    # save dataframe to csv\n",
    "    combineresults_df.to_csv('nflcombineresults_2000_2020.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
